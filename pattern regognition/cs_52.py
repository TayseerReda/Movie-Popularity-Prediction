# -*- coding: utf-8 -*-
"""CS_52.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17n5dlPya76M1kbtAb_CgBFs72eFhce-c
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.linear_model import LinearRegression , Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import json
import nltk
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from collections import Counter
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.preprocessing import PolynomialFeatures
from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Load the dataset
dataset = pd.read_csv('movies-regression-dataset.csv')

# Remove irrelevant columns
irrelevant_col=['id','homepage', 'overview', 'tagline', 'title','original_title']
dataset = dataset.drop(columns=irrelevant_col)

# Drop NaN values in categorical columns
categorical_cols = ['genres', 'keywords', 'original_language','production_companies','release_date','spoken_languages','status']
dataset[categorical_cols] = dataset[categorical_cols].dropna()

# Replace NaN values in numerical columns with their mean
numerical_cols = ['budget', 'viewercount', 'revenue', 'runtime', 'vote_count']
col_means = []
for col in numerical_cols:
    col_mean = dataset[col].mean()
    dataset[col] = dataset[col].fillna(col_mean)
    col_means.append(col_mean)



# Count the number of missing values in each column
na_counts = dataset.isna().sum()

# Print the number of missing values in each column
print(na_counts)

# Create an instance of the LabelEncoder class
labelencoder = LabelEncoder()

# Fit the label encoder to the "original_language" column
#labelencoder.fit(dataset["original_language"])
# Transform the "original_language" column using the label encoder
#dataset["original_language"] = labelencoder.transform(dataset["original_language"])

# Fit the label encoder to the "status" column
labelencoder.fit(dataset["status"])
# Transform the "status" column using the label encoder
dataset["status"] = labelencoder.transform(dataset["status"])

# Print the updated dataframe
#print(dataset[status])

#get unique values
unique_values = dataset['original_language'].unique()

#encoding 'original_language' column with one hot encoding baiscally turning every unique value of original_language into a new column with binary value
encoded_df = pd.get_dummies(dataset['original_language'], prefix='', prefix_sep='')

# merge the encoded_df with the original df
dataset = pd.concat([dataset, encoded_df], axis=1)
dataset = dataset.drop(['original_language'], axis=1)

#Function for extracting text
def extract_text(x):
    column_text = []
    for i in json.loads(x):
        column_text.append(i['name'])
    return column_text

# Extract genres from 'genres' column
genres = dataset['genres'].apply(extract_text)

# Count the frequency of each keyword
keyword_counts = Counter(genres.explode())

# Get the top 60 most common genres
top_genres = dict(keyword_counts.most_common())


# Create a new dataframe with one-hot encoded top genres
one_hot_df = pd.DataFrame()
for keyword in top_genres:
    one_hot_df[keyword] = genres.apply(lambda x: 1 if keyword in x else 0)

# Drop 'genres' column from dataset
dataset = dataset.drop(['genres'], axis=1)

# Concatenate the one-hot encoded dataframe with the original dataset
dataset = pd.concat([dataset, one_hot_df], axis=1)
print(top_genres)

# Extract keywords from 'keywords' column
keywords = dataset['keywords'].apply(extract_text)

# Count the frequency of each keyword
keyword_counts = Counter(keywords.explode())

# Get the top 60 most common keywords
top_keywords = dict(keyword_counts.most_common(60))


# Create a new dataframe with one-hot encoded top keywords
one_hot_df = pd.DataFrame()
for keyword in top_keywords:
    one_hot_df[keyword] = keywords.apply(lambda x: 1 if keyword in x else 0)

# Drop 'keywords' column from dataset
dataset = dataset.drop(['keywords'], axis=1)

# Concatenate the one-hot encoded dataframe with the original dataset
dataset = pd.concat([dataset, one_hot_df], axis=1)
print(top_keywords)

# Extract production_companies from 'production_companies' column
production_companies = dataset['production_companies'].apply(extract_text)

# Count the frequency of each keyword
keyword_counts = Counter(production_companies.explode())

# Get the top 60 most common production_companies
top_production_companies = dict(keyword_counts.most_common(60))


# Create a new dataframe with one-hot encoded top production_companies
one_hot_df = pd.DataFrame()
for keyword in top_production_companies:
    one_hot_df[keyword] = production_companies.apply(lambda x: 1 if keyword in x else 0)

# Drop 'production_companies' column from dataset
dataset = dataset.drop(['production_companies'], axis=1)

# Concatenate the one-hot encoded dataframe with the original dataset
dataset = pd.concat([dataset, one_hot_df], axis=1)
print(top_production_companies)

# Extract production_countries from 'production_countries' column
production_countries = dataset['production_countries'].apply(extract_text)

# Count the frequency of each keyword
keyword_counts = Counter(production_countries.explode())

# Get the top 60 most common production_countries
top_production_countries = dict(keyword_counts.most_common(60))


# Create a new dataframe with one-hot encoded top production_countries
one_hot_df = pd.DataFrame()
for keyword in top_production_countries:
    one_hot_df[keyword] = production_countries.apply(lambda x: 1 if keyword in x else 0)

# Drop 'production_countries' column from dataset
dataset = dataset.drop(['production_countries'], axis=1)

# Concatenate the one-hot encoded dataframe with the original dataset
dataset = pd.concat([dataset, one_hot_df], axis=1)
print(top_production_countries)

# Extract spoken_languages from 'spoken_languages' column
spoken_languages = dataset['spoken_languages'].apply(extract_text)

# Count the frequency of each keyword
keyword_counts = Counter(spoken_languages.explode())

# Get the top 60 most common spoken_languages
top_spoken_languages = dict(keyword_counts.most_common(60))


# Create a new dataframe with one-hot encoded top spoken_languages
one_hot_df = pd.DataFrame()
for keyword in top_spoken_languages:
    one_hot_df[keyword] = spoken_languages.apply(lambda x: 1 if keyword in x else 0)

# Drop 'spoken_languages' column from dataset
dataset = dataset.drop(['spoken_languages'], axis=1)

# Concatenate the one-hot encoded dataframe with the original dataset
dataset = pd.concat([dataset, one_hot_df], axis=1)
print(top_spoken_languages)

# Convert release_date column to datetime
dataset['release_date'] = pd.to_datetime(dataset['release_date'], format='%m/%d/%Y')

# Create new columns using pd.concat()
new_cols = pd.concat([dataset['release_date'].dt.year, dataset['release_date'].dt.month, dataset['release_date'].dt.day], axis=1)
new_cols.columns = ['year', 'month', 'day']

# Concatenate the new columns with the original DataFrame
dataset = pd.concat([dataset, new_cols], axis=1)

# Drop the release_date column
dataset = dataset.drop(['release_date'], axis=1)

# Compute the correlation matrix
corr_matrix = dataset.corr()

# Find the correlations with vote_average
vote_corr = corr_matrix['vote_average'].sort_values(ascending=False)

# Save the feature names with correlation > 0.09 in a list
selected_features = list(vote_corr[vote_corr > 0.09].index)

# Select the features and target variable
X = dataset[selected_features].drop(columns=['vote_average'])
y = dataset['vote_average']

# Print the selected features
print(selected_features)


#Top 50% Correlation training features with the Value
top_feature = corr_matrix.index[abs(corr_matrix['vote_average'])>0.09]
#Correlation plot
plt.subplots(figsize=(12, 8))
top_corr = dataset[top_feature].corr()
sns.heatmap(top_corr, annot=True)
plt.show()
top_feature = top_feature.delete(-1)

#compute train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#scaling X_train

#list of numerical that need scaling
numrical_scale=['runtime','vote_count','viewercount','revenue','month']

# Create a StandardScaler object
scaler = StandardScaler()

# Fit the scaler to the numerical columns
scaler.fit(X_train[numrical_scale])

# Transform the numerical columns using the scaler
scaled_numrical_scale = scaler.transform(X_train[numrical_scale])

# Replace the original numerical columns with the scaled ones
X_train[numrical_scale] = scaled_numrical_scale

#scaling X_test

# Fit the scaler to the numerical columns
scaler.fit(X_test[numrical_scale])

# Transform the numerical columns using the scaler
scaled_numrical_scale = scaler.transform(X_test[numrical_scale])

# Replace the original numerical columns with the scaled ones
X_test[numrical_scale] = scaled_numrical_scale

# Save the data using pickle
with open("preprocessor.pkl", "wb") as f:
    pickle.dump(irrelevant_col, f) #list
    pickle.dump(categorical_cols, f) #list
    pickle.dump(numerical_cols, f) #list
    pickle.dump(col_means, f) #list
    pickle.dump(scaler, f) #standard scalar
    pickle.dump(labelencoder, f) #label encoder
    pickle.dump(top_genres, f) #dictionary
    pickle.dump(top_keywords, f) #dictionary
    pickle.dump(top_production_companies, f) #dictionary
    pickle.dump(top_production_countries, f) #dictionary
    pickle.dump(top_spoken_languages, f) #dictionary
    pickle.dump(selected_features, f) #list
    pickle.dump(numrical_scale, f) #list
    pickle.dump(unique_values,f)#list

#function of preprocessing for nwew data
def load_preprocessor(df):
    with open("preprocessor.pkl", "rb") as f:
        irrelevant_col = pickle.load(f)
        categorical_cols = pickle.load(f)
        numerical_cols = pickle.load(f)
        col_means = pickle.load(f)
        scaler = pickle.load(f)
        labelencoder = pickle.load(f)
        top_genres = pickle.load(f)
        top_keywords = pickle.load(f)
        top_production_companies = pickle.load(f)
        top_production_countries = pickle.load(f)
        top_spoken_languages = pickle.load(f)
        selected_features = pickle.load(f)
        numrical_scale=pickle.load(f)
        unique_values=pickle.load(f)

    # Remove irrelevant columns from the DataFrame
    df = df.drop(columns=irrelevant_col)

    # Drop NaN values in categorical columns
    df[categorical_cols] = df[categorical_cols].dropna()

    # Replace NaN values in numerical columns with their mean
    for col in numerical_cols:
       df[col] = df[col].fillna(col_mean)

    ###label encoder###

    # Fit the label encoder to the "original_language" column
    #labelencoder.fit(df["original_language"])
    # Transform the "original_language" column using the label encoder
    #df["original_language"] = labelencoder.transform(df["original_language"])

    # Fit the label encoder to the "status" column
    labelencoder.fit(df["status"])
    # Transform the "status" column using the label encoder
    df["status"] = labelencoder.transform(df["status"])

    ###############################################################


    ### Add one-hot encoded columns for the top genres###

    encoded_df = pd.get_dummies(df['original_language'], prefix='', prefix_sep='')

    #drop any new columns
    encoded_df=drop_columns_except(encoded_df,unique_values)
    # merge the encoded_df with the original df
    df = pd.concat([df, encoded_df], axis=1)
    df = df.drop(['original_language'], axis=1)

    ###############################################################

    ###############################################################


    ### Add one-hot encoded columns for the top genres###

    # Extract genres from 'genres' column
    genres = df['genres'].apply(extract_text)

    # Create a new dataframe with one-hot encoded top genres
    one_hot_df = pd.DataFrame()
    for keyword in top_genres:
        one_hot_df[keyword] = genres.apply(lambda x: 1 if keyword in x else 0)

    # Drop 'genres' column from df
    df = df.drop(['genres'], axis=1)

    # Concatenate the one-hot encoded dataframe with the original df
    df = pd.concat([df, one_hot_df], axis=1)

    ###############################################################

    ### Add one-hot encoded columns for the top keywords###

    # Extract genres from 'keywords' column
    keywords = df['keywords'].apply(extract_text)

    # Create a new dataframe with one-hot encoded top keywords
    one_hot_df = pd.DataFrame()
    for keyword in top_keywords:
        one_hot_df[keyword] = keywords.apply(lambda x: 1 if keyword in x else 0)

    # Drop 'keywords' column from df
    df = df.drop(['keywords'], axis=1)

    # Concatenate the one-hot encoded dataframe with the original df
    df = pd.concat([df, one_hot_df], axis=1)


    ################################################################

    ### Add one-hot encoded columns for the top production companies###

    # Extract genres from 'production_companies' column
    companies = df['production_companies'].apply(extract_text)

    # Create a new dataframe with one-hot encoded top production_companies
    one_hot_df = pd.DataFrame()
    for keyword in top_production_companies:
        one_hot_df[keyword] = companies.apply(lambda x: 1 if keyword in x else 0)

    # Drop 'production_companies' column from df
    df = df.drop(['production_companies'], axis=1)

    # Concatenate the one-hot encoded dataframe with the original df
    df = pd.concat([df, one_hot_df], axis=1)


    ################################################################

    # Add one-hot encoded columns for the top production countries

    # Extract genres from 'production_countries' column
    countries = df['production_countries'].apply(extract_text)

    # Create a new dataframe with one-hot encoded top production_countries
    one_hot_df = pd.DataFrame()
    for keyword in top_production_countries:
        one_hot_df[keyword] = countries.apply(lambda x: 1 if keyword in x else 0)

    # Drop 'production_countries' column from df
    df = df.drop(['production_countries'], axis=1)

    # Concatenate the one-hot encoded dataframe with the original df
    df = pd.concat([df, one_hot_df], axis=1)
    
    
    ################################################################

    # Add one-hot encoded columns for the top spoken languages
    
    # Extract genres from 'spoken_languages' column
    languages = df['spoken_languages'].apply(extract_text)

    # Create a new dataframe with one-hot encoded top spoken_languages
    one_hot_df = pd.DataFrame()
    for keyword in top_spoken_languages:
        one_hot_df[keyword] = languages.apply(lambda x: 1 if keyword in x else 0)

    # Drop 'spoken_languages' column from df
    df = df.drop(['spoken_languages'], axis=1)

    # Concatenate the one-hot encoded dataframe with the original df
    df = pd.concat([df, one_hot_df], axis=1)

    ################################################################

    # Convert release_date column to datetime
    df['release_date'] = pd.to_datetime(df['release_date'], format='%m/%d/%Y')

    # Create new columns using pd.concat()
    new_cols = pd.concat([df['release_date'].dt.year, df['release_date'].dt.month, df['release_date'].dt.day], axis=1)
    new_cols.columns = ['year', 'month', 'day']

    # Concatenate the new columns with the original DataFrame
    df = pd.concat([df, new_cols], axis=1)

    # Drop the release_date column
    df = df.drop(['release_date'], axis=1)

    # Select the relevant columns for modeling
    df = df[selected_features]

    ####### Scale the numerical columns using StandardScaler######
    
    # Fit the scaler to the numerical columns
    scaler.fit(df[numrical_scale])

    # Transform the numerical columns using the scaler
    scaled_numerical_cols = scaler.transform(df[numrical_scale])

    # Replace the original numerical columns with the scaled ones
    df[numrical_scale] = scaled_numerical_cols
    
    ###############################################################

    return df

models=["Polynomial","Linear","Ridge"]
plt_mean_squared_error=[]

poly_features = PolynomialFeatures(degree=2, include_bias=False) 
X_train_poly = poly_features.fit_transform(X_train)
poly_model = LinearRegression()
poly_model.fit(X_train_poly, y_train)
prediction = poly_model.predict(poly_features.fit_transform(X_test))

pred_train=  poly_model.predict(poly_features.fit_transform(X_train))



print('R2_score for training',r2_score(y_train, pred_train))
print('R2_score for testing',r2_score(y_test,prediction))

MSE= metrics.mean_squared_error(y_test, prediction)
plt_mean_squared_error.append(MSE)
print('Mean Square Error for testing',MSE)
print('Mean Square Error for training',metrics.mean_squared_error(y_train, pred_train))



#df1=pd.DataFrame({'Actual': y_test,'predicted': prediction}).reset_index()
#sns.lmplot(x="predicted", y="Actual", data=df1);


#plt.scatter(prediction,y_test, color="green")
#plt.plot(y_test,prediction, color="red") 
#plt.xlabel("predicted")
#plt.ylabel("Actual")


#pickle
with open('poly_reg_model.pkl', 'wb') as f:
    pickle.dump(poly_model, f)


with open('poly_reg_model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

model = LinearRegression()
model.fit(X_train, y_train)
prediction = model.predict(X_test)

pred_train=  model.predict(X_train)

print('R2_score for training',r2_score(y_train, pred_train))
print('R2_score for testing',r2_score(y_test,prediction))

MSE= metrics.mean_squared_error(y_test, prediction)
plt_mean_squared_error.append(MSE)
print('Mean Square Error for testing',MSE)
print('Mean Square Error for training',metrics.mean_squared_error(y_train, pred_train))



with open('lin_reg_model.pkl', 'wb') as f:
    pickle.dump(poly_model, f)


with open('lin_reg_model.pkl', 'rb') as f:
    loaded_model = pickle.load(f) 

#y_pred_new = loaded_model.predict(il data il gdeda)

from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
prediction = ridge.predict(X_test)

pred_train= ridge.predict(X_train)

print('R2_score for training',r2_score(y_train, pred_train))
print('R2_score for testing',r2_score(y_test,prediction))

MSE= metrics.mean_squared_error(y_test, prediction)
plt_mean_squared_error.append(MSE)
print('Mean Square Error for testing',MSE)
print('Mean Square Error for training',metrics.mean_squared_error(y_train, pred_train))



with open('rige_reg_model.pkl', 'wb') as f:
    pickle.dump(poly_model, f)


with open('rige_reg_model.pkl', 'rb') as f:
    loaded_model = pickle.load(f) 
#y_pred_new = loaded_model.predict(il data il gdeda)

model_lasso = Lasso(alpha=0.5)
model_lasso.fit(X_train, y_train)
train_lasso= model_lasso.predict(X_train)
test_lasso= model_lasso.predict(X_test)

print(np.sqrt( metrics.mean_squared_error(y_test,test_lasso)))
#print(r2_score(y_test, test_lasso))
# perform cross-validation on the training set
scores = cross_val_score(model_lasso, X_train, y_train, cv=10, scoring='neg_mean_squared_error')

  # print the cross-validation scores
print('Cross-validation scores:', scores)
print('Mean squared error:', -scores.mean())
lasso_data={'model': model_lasso,'alpha': 1.0}
with open(f'lasso.pickle', 'wb') as f:
  pickle.dump(lasso_data, f)

randomforest = RandomForestRegressor(n_estimators=10)
#n_estimators: the no. of decision trees(changing the parameters doesn't affect performance but increases computations)
randomforest.fit(X_train, y_train)

y_pred = model.predict(X_test)

# evaluate the performance of the model
mse = metrics.mean_squared_error(y_test, y_pred)
print('Mean squared error:', mse)
# perform cross-validation on the training set
scores = cross_val_score(randomforest, X_train, y_train, cv=10, scoring='neg_mean_squared_error')

  # print the cross-validation scores
print('Cross-validation scores:', scores)
print('Mean squared error:', -scores.mean())

#pickling:
randomforest_data={'model': randomforest,'n_estimators': 10}
with open(f'rf.pickle', 'wb') as f:
  pickle.dump(randomforest_data, f)

def drop_columns_except(df, columns_to_keep):
    columns_to_drop = [col for col in df.columns if col not in columns_to_keep]
    df = df.drop(columns=columns_to_drop)
    return df

test = pd.read_csv('movies-regression-dataset.csv')
result = load_preprocessor(test)
print(result)